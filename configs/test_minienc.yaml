model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  modal: audio
  encoder_type: finetune

  # LLM settings
  llm_model_name: llama3.2-3B
  llm_model: meta-llama/Llama-3.2-3B
  llm_model_path: null
  llm_type: decoder_only
  llm_dim: 3072

  # Mini audio encoder + projector (linear head)
  frontend: mini_audio_encoder_projector
  projector: linear                 # 'linear' or 'query'
  mel_size: 80
  mel_input_norm: true              # per-bin LayerNorm in the module
  d_model: 384
  mel_time_stride: 8                # encoder stride (ceil(T/8) tokens)
  conv_layers: 2                    # DS-Conv blocks (dilations 1,2)
  tfm_layers: 2                     # lightweight Transformer depth
  nhead: 6                          # attention heads in encoder blocks
  add_posenc: true                  # sinusoidal PE after pooling
  dropout: 0.1                      # encoder dropout
  mel_dropout: 0.0                  # projector dropout (keep 0.0 for determinism in tests)

train:
  model_name: ASRLLM
  run_validation: true
  batch_size: 8
  batching_strategy: packing
  context_length: 384
  gradient_accumulation_steps: 2
  grad_clip: 1.0
  num_epochs: 25
  num_workers: 8
  validation_interval: 1000
  lr: 1.0e-4
  weight_decay: 0.01
  gamma: 0.85
  seed: 42
  use_fp16: false
  mixed_precision: true
  val_batch_size: 8

  # Training options
  use_peft: false
  output_dir: outputs_3b_mini_enc_linear/
  freeze_layers: false
  num_freeze_layers: 0
  quantization: false
  one_gpu: true
  save_model: true
  use_fast_kernels: false
  freeze_llm: true                 # projector/encoder-only check, unfreeze later


model:
  model_name: ASRLLM
  file: models/model.py:ASRLLM
  modal: audio
  encoder_type: finetune

  # LLM settings
  llm_model_name: llama3.2-3B
  llm_model: meta-llama/Llama-3.2-3B
  llm_model_path: null
  llm_type: decoder_only
  llm_dim: 3072

  # Mini audio encoder + projector (linear head)
  projector: mini_audio_encoder
  projector_type: linear                 # 'linear' or 'query'
  mel_size: 80
  mel_input_norm: true              # per-bin LayerNorm in the module
  d_model: 384
  mel_time_stride: 8                # encoder stride (ceil(T/8) tokens)
  conv_layers: 2                    # DS-Conv blocks (dilations 1,2)
  tfm_layers: 2                     # lightweight Transformer depth
  nhead: 6                          # attention heads in encoder blocks
  add_posenc: true                  # sinusoidal PE after pooling
  dropout: 0.1                      # encoder dropout
  mel_dropout: 0.0                  # projector dropout (keep 0.0 for determinism in tests)

train:
  model_name: ASRLLM
  run_validation: true
  batch_size: 8
  batching_strategy: packing
  context_length: 384
  gradient_accumulation_steps: 2
  grad_clip: 1.0
  num_epochs: 25
  num_workers: 8
  validation_interval: 1000
  lr: 1.0e-4
  weight_decay: 0.01
  gamma: 0.85
  seed: 42
  use_fp16: false
  mixed_precision: true
  val_batch_size: 8

  # Training options
  use_peft: false
  output_dir: outputs_3b_mini_enc_linear/
  freeze_layers: false
  num_freeze_layers: 0
  quantization: false
  one_gpu: true
  save_model: true
  use_fast_kernels: false
  freeze_llm: true                 # projector/encoder-only check, unfreeze later

scheduler: 
  name: cosine_with_warmup
  total_training_steps: 65010
  warmup_steps: 1950

# Early Stopping
early_stopping: 
  monitor: val/loss
  mode: min 
  patience: 2 
  min_delta: 0.001

data:
  dataset: speech_dataset
  file: datamodules/datasets.py:get_speech_dataset
  train_data_path: data/train-clean-20.jsonl
  val_data_path: data/dev-clean.jsonl
  test_data_path: data/test-clean.jsonl
  train_split: train
  val_split: val
  test_split: test
  prompt: null
  data_path: null
  max_words: null
  max_mel: null
  fix_length_audio: -1
  inference_mode: false
  input_type: mel
  mel_size: 80
  normalize: false

log:
  use_wandb: true
  wandb_dir: ./wandb
  wandb_entity_name: ASRLLM
  wandb_project_name: ASRLLM_llama_3B_20h
  wandb_exp_name: exp_3b_mini_enc_linear
  log_dir: ./logs
  log_filename: training_3b_mini_enc_linear.log
  log_interval: 50
